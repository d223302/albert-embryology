{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!source activate sp-env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, BertForPreTraining, WordpieceTokenizer, BertConfig,\\\n",
    "                         AlbertTokenizer, AlbertModel, AlbertForMaskedLM, AlbertConfig\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlbertForMaskedLM(\n",
       "  (albert): AlbertModel(\n",
       "    (embeddings): AlbertEmbeddings(\n",
       "      (word_embeddings): Embedding(30000, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "    (encoder): AlbertTransformer(\n",
       "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
       "      (albert_layer_groups): ModuleList(\n",
       "        (0): AlbertLayerGroup(\n",
       "          (albert_layers): ModuleList(\n",
       "            (0): AlbertLayer(\n",
       "              (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (attention): AlbertAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0, inplace=False)\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              )\n",
       "              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (pooler_activation): Tanh()\n",
       "  )\n",
       "  (predictions): AlbertMLMHead(\n",
       "    (LayerNorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (dense): Linear(in_features=768, out_features=128, bias=True)\n",
       "    (decoder): Linear(in_features=128, out_features=30000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AlbertTokenizer(vocab_file = '/work/dcml0714/albert/albert_base/30k-clean.model')\n",
    "config = AlbertConfig.from_json_file('/work/dcml0714/albert/albert_base/albert_config.json')\n",
    "#model = AlbertModel.from_pretrained(pretrained_model_name_or_path = 'albert-base-v1',\n",
    "#                                    output_hidden_states = True, \n",
    "#                                    output_attentions = True)\n",
    "config.output_hidden_states = True\n",
    "config.output_attentions = True\n",
    "model = AlbertForMaskedLM.from_pretrained(pretrained_model_name_or_path = None,\n",
    "                                    config = config,\n",
    "                                    state_dict = torch.load('/work/dcml0714/albert/pytorch_model/pytorch_model_600000.bin'))\n",
    "model.eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = 'Donal Trump was born in New York.'\n",
    "indexed_tokens = tokenizer.encode(line)\n",
    "indexed_tokens[-2] = 4\n",
    "indexed_tokens[-3] = 4\n",
    "print(tokenizer.convert_ids_to_tokens(indexed_tokens)[1:-1])\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    \n",
    "with torch.no_grad():\n",
    "  loss, prediction_scores = model(tokens_tensor.cuda(),masked_lm_labels = tokens_tensor.cuda())[:2]\n",
    "\n",
    "prediction_scores = prediction_scores.squeeze().argmax(-1).cpu().numpy()[1:-1]\n",
    "indexed_tokens = np.asarray(indexed_tokens[1:-1])\n",
    "prediction_text = tokenizer.convert_ids_to_tokens(prediction_scores)\n",
    "print(prediction_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
