{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!source activate sp-env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, BertForPreTraining, WordpieceTokenizer, BertConfig,\\\n",
    "                         AlbertTokenizer, AlbertModel, AlbertForMaskedLM, AlbertConfig\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlbertForMaskedLM(\n",
       "  (albert): AlbertModel(\n",
       "    (embeddings): AlbertEmbeddings(\n",
       "      (word_embeddings): Embedding(30000, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "    (encoder): AlbertTransformer(\n",
       "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
       "      (albert_layer_groups): ModuleList(\n",
       "        (0): AlbertLayerGroup(\n",
       "          (albert_layers): ModuleList(\n",
       "            (0): AlbertLayer(\n",
       "              (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (attention): AlbertAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0, inplace=False)\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              )\n",
       "              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (pooler_activation): Tanh()\n",
       "  )\n",
       "  (predictions): AlbertMLMHead(\n",
       "    (LayerNorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (dense): Linear(in_features=768, out_features=128, bias=True)\n",
       "    (decoder): Linear(in_features=128, out_features=30000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AlbertTokenizer(vocab_file = '/work/dcml0714/albert/albert_base/30k-clean.model')\n",
    "config = AlbertConfig.from_json_file('/work/dcml0714/albert/albert_base/albert_config.json')\n",
    "#model = AlbertModel.from_pretrained(pretrained_model_name_or_path = 'albert-base-v1',\n",
    "#                                    output_hidden_states = True, \n",
    "#                                    output_attentions = True)\n",
    "config.output_hidden_states = True\n",
    "config.output_attentions = True\n",
    "model = AlbertForMaskedLM.from_pretrained(pretrained_model_name_or_path = None,\n",
    "                                    config = config,\n",
    "                                    state_dict = torch.load('/work/dcml0714/albert/pytorch_model/pytorch_model_650000.bin'))\n",
    "model.eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = \"I w\"\n",
    "  indexed_tokens = tokenizer.encode(line)  \n",
    "  segment_ids = [0 for _ in indexed_tokens]\n",
    "  tokens_tensor = torch.tensor([indexed_tokens])\n",
    "  segments_tensors = torch.tensor([segment_ids])\n",
    "  seq_len = len(segment_ids)\n",
    "    \n",
    "  with torch.no_grad():\n",
    "    hidden = model(tokens_tensor.cuda(), token_type_ids = segments_tensors.cuda())\n",
    "    final_output, cls_output, hidden_states, attn_weight,\\\n",
    "      values, context, attn_after_LN, FFL_before_LN = hidden = hidden\n",
    "    \n",
    "  for i in range(seq_len):\n",
    "    if str(indexed_tokens[i]) not in self_word[0]:\n",
    "      for layer in range(13):\n",
    "        hidden_word[layer][str(indexed_tokens[i])] = [hidden_states[layer].squeeze().detach().cpu().numpy()[i]] \n",
    "        if layer == 12: break    \n",
    "        self_word[layer][str(indexed_tokens[i])] = [attn_after_LN[layer].squeeze().detach().cpu().numpy()[i]]\n",
    "        ff_word[layer][str(indexed_tokens[i])] = [FFL_before_LN[layer].squeeze().detach().cpu().numpy()[i]]\n",
    "        context_word[layer][str(indexed_tokens[i])] = [context[layer].squeeze().detach().cpu().numpy()[i]]\n",
    "    else:\n",
    "      for layer in range(13):\n",
    "        hidden_word[layer][str(indexed_tokens[i])].append(hidden_states[layer].squeeze().detach().cpu().numpy()[i])  \n",
    "        if layer == 12: break\n",
    "        self_word[layer][str(indexed_tokens[i])].append(attn_after_LN[layer].squeeze().detach().cpu().numpy()[i])\n",
    "        ff_word[layer][str(indexed_tokens[i])].append(FFL_before_LN[layer].squeeze().detach().cpu().numpy()[i])\n",
    "        context_word[layer][str(indexed_tokens[i])].append(context[layer].squeeze().detach().cpu().numpy()[i])  \n",
    "        \n",
    "  for i in range(13):\n",
    "    cos_data['ani']['all_hidden'][i].extend(hidden_states[i].squeeze().detach().cpu().numpy())\n",
    "    cos_data['intra']['all_hidden'][i].append(intra_sent_cosine(hidden_states[i].squeeze().detach().cpu().numpy()))\n",
    "    if i ==12: break \n",
    "      \n",
    "    # For intra-sentence cosine\n",
    "    cos_data['intra']['all_attn_after_LN'][i].append(intra_sent_cosine(attn_after_LN[i].squeeze().detach().cpu().numpy()))\n",
    "    cos_data['intra']['all_FFL_before_LN'][i].append(intra_sent_cosine(FFL_before_LN[i].squeeze().detach().cpu().numpy()))\n",
    "    cos_data['intra']['all_context'][i].append(intra_sent_cosine(context[i].squeeze().detach().cpu().numpy()))\n",
    "        \n",
    "    # For anisoropy  \n",
    "    cos_data['ani']['all_attn_after_LN'][i].extend(attn_after_LN[i].squeeze().detach().cpu().numpy())\n",
    "    cos_data['ani']['all_FFL_before_LN'][i].extend(FFL_before_LN[i].squeeze().detach().cpu().numpy())  \n",
    "    cos_data['ani']['all_context'][i].extend(context[i].squeeze().detach().cpu().numpy())  \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(x):\n",
    "  x = torch.from_numpy(x).cuda()\n",
    "  norm = torch.norm(x, dim = -1, keepdim = True)\n",
    "  cos_sim = (torch.matmul(x, x.T) / (torch.matmul(norm, norm.T) + 1e-10)).triu(diagonal = 1).cpu().numpy()\n",
    "  return cos_sim.sum() / np.triu(np.ones_like(cos_sim), 1).sum()\n",
    "\n",
    "for vocab in self_word[0].keys():\n",
    "  if len(self_word[0][vocab]) < 5: continue\n",
    "  for i in range(13):\n",
    "    cos_data['self']['all_hidden'][i].append(cosine(np.stack(hidden_word[i][vocab][:30])))\n",
    "    if i == 12: break\n",
    "    cos_data['self']['all_context'][i].append(cosine(np.stack(context_word[i][vocab][:30])))    \n",
    "    cos_data['self']['all_attn_after_LN'][i].append(cosine(np.stack(self_word[i][vocab][:30])))\n",
    "    cos_data['self']['all_FFL_before_LN'][i].append(cosine(np.stack(ff_word[i][vocab][:30])))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in cos_data['ani']:\n",
    "   for i in range(13 if 'hidden' in key else 12): \n",
    "     cos_data['ani'][key][i] = cosine(np.asarray(cos_data['ani'][key][i][:1000])) \n",
    "for key in cos_data['intra']:\n",
    "   for i in range(13 if 'hidden' in key else 12): \n",
    "     cos_data['intra'][key][i] = np.asarray(cos_data['intra'][key][i]).mean() \n",
    "for key in cos_data['self']:\n",
    "   for i in range(13 if 'hidden' in key else 12): \n",
    "     cos_data['self'][key][i] = np.asarray(cos_data['self'][key][i]).mean()         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "line_type = ['o', '^', 's', 'x']\n",
    "for i, output in enumerate(['all_attn_after_LN', 'all_FFL_before_LN', 'all_context', 'all_hidden']):\n",
    "  plt.plot(np.arange(13 if 'hidden' in output else 12), \n",
    "           np.asarray(cos_data['self'][output]) - np.asarray(cos_data['ani'][output]), \n",
    "           label = 'self similarity', color = 'red', marker = line_type[i], linestyle = 'dashed')\n",
    "  plt.plot(np.arange(13 if 'hidden' in output else 12),\n",
    "           np.asarray(cos_data['intra'][output]) - np.asarray(cos_data['ani'][output]), \n",
    "           label = 'intrasentence similarity', color = 'g', marker = line_type[i], linestyle = 'dashed')\n",
    "plt.grid(True)\n",
    "plt.title('Contextualize ')\n",
    "plt.savefig('context.pdf')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
